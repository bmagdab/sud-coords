% how was training conducted -- i used stanza scripts, i used those parameters bc idk
% conversion code analysis -- where do SUD corpora come from?
% trained models -- combined, comparative, spoken -- purpose of each of those and how i chose corpora for them
% model comparison UD vs SUD

The data used in this work is based on the Corpus of Contemporary American English. The corpus consists of raw texts collected in a span of 30 years (1990 -- 2019) representing 8 styles: academic, fiction, newspapers, magazines, TV/movies, websites, blogs and spoken. For the analysis of coordinations to be possible, first the texts have to be annotated syntactically. The Stanza parser chosen for this task by default annotates in the Universal Dependencies scheme, but as was explained earlier, the Surface-syntactic Universal Dependencies scheme is preferred here. The parser therefore had to be trained to annotate in the SUD scheme. 

The current chapter has two sections: the first one describes the process of training the parsing models that created the syntactic annotation. The second one describes the procedure of finding coordinate structures in parsed sentences and creating a table with data ready for analysis. 

\section{Parser training}
Training was conducted using scritps available on github.\footnote{\url{https://github.com/stanfordnlp/stanza-train}} For the parser to learn annotation, there needs to be already annotated data. The Surface-syntactic Universal Dependencies is a much smaller project than Universal Dependencies, therefore there are not many corpora annotated natively in this scheme. There is however a conversion code that makes it possible to transform any UD data to SUD.\footnote{\url{https://github.com/surfacesyntacticud/tools/blob/v2.12/converter/grs/UD_to_SUD.grs}} 

HERE WILL BE A DESCRIPTION (NOT SURE HOW DETAILED) OF THE CONVERSION CODE

Dependency trees as shown in previous chapters, though possibly comprehensible for people, are not written in a way that is easily understandable by computer programs. Hence, \cite{buchholz-marsi-2006-conll} created the CoNLL-X format. The exact purpose was to compare parser outputs in a dependency parsing shared task. Today it is widely used for representing dependency trees in a plain-text form. The UD project adapted the format for their needs by replacing some of the information included in CoNLL-X. The adapted version is called CoNLL-U and in (\ref{ex:conllu}) there is an example of a SUD dependency tree presented as a tree and in the CoNLL-U format. 

\begin{adjustwidth}{-70pt}{50pt}
\begin{exe}
	\ex
	\label{ex:conllu}
	\begin{dependency}[show label]
	\begin{deptext}[column sep=0.4cm]
	This\& magma\& often\& does\& not\& reach\& the\& surface\& but\& cools\& at\& depth.\\
	 \end{deptext} 
	\depedge{2}{1}{det} 
	\depedge[show label]{4}{2}{subj} 
	\depedge[show label]{4}{3}{mod} 
	\deproot[show label, edge height=2cm]{4}{root} 
	\depedge{4}{5}{mod} 
	\depedge{4}{6}{comp:aux} 
	\depedge{8}{7}{det} 
	\depedge{6}{8}{comp:obj} 
	\depedge[show label]{10}{9}{cc} 
	\depedge[show label, edge height=1.5cm]{4}{10}{conj} 
	\depedge[show label]{10}{11}{udep} 
	\depedge{11}{12}{comp:obj} 
	\end{dependency}
	
	\input{conllutable}
\end{exe}
\end{adjustwidth}

\section{Data extraction}