The data used in this work is based on the Corpus of Contemporary American English. The corpus consists of raw texts collected in a span of 30 years (1990 -- 2019) representing 8 styles: academic, fiction, newspapers, magazines, TV/movies, websites, blogs and spoken data. For the analysis of coordinations to be possible, first the texts have to be annotated syntactically -- here the Stanza parser was chosen for this task. The default parsing model provided for English annotates in the Universal Dependencies scheme, but as was explained earlier, the Surface-syntactic Universal Dependencies scheme is preferred here. The parser therefore had to be trained to annotate in the SUD scheme. 

The current chapter has two sections: the first one describes the process of training the parsing models that created the syntactic annotation. The second one describes the procedure of finding coordinate structures in parsed sentences and creating a table with data ready for analysis. 

\section{Parser training}
Training was conducted using scritps made available on github by the Stanza developers.\footnote{\url{https://github.com/stanfordnlp/stanza-train}} For the parser to learn annotation, there needs to be already annotated data. The Surface-syntactic Universal Dependencies is a much smaller project than Universal Dependencies, therefore there are not many corpora annotated natively in this scheme. Because of that, a set of graph conversion rules was developed.\footnote{\url{https://github.com/surfacesyntacticud/tools/blob/v2.12/converter/grs/UD_to_SUD.grs}} Using this conversion code, almost all of the UD corpora have been translated to SUD. 

Dependency trees as shown in previous chapters, though possibly comprehensible for people, are not written in a way that is easily understandable by computer programs, including parsers. Hence \cite{buchholz-marsi-2006-conll} created the CoNLL-X format. The exact purpose was to compare parser outputs in a dependency parsing shared task. Today it is widely used for representing dependency trees in a plain-text form. The UD project adapted the format to their needs by replacing some of the information included in CoNLL-X and thus creating the CoNLL-U format. In (\ref{ex:conllu}) there is an example of a SUD dependency tree presented as a tree and in the CoNLL-U format. 

\begin{adjustwidth}{-70pt}{50pt}
\begin{exe}
	\ex
	\label{ex:conllu}
	\begin{dependency}[show label]
	\begin{deptext}[column sep=0.4cm]
	This\& magma\& often\& does\& not\& reach\& the\& surface\& but\& cools\& at\& depth.\\
	 \end{deptext} 
	\depedge{2}{1}{det} 
	\depedge[show label]{4}{2}{subj} 
	\depedge[show label]{4}{3}{mod} 
	\deproot[show label, edge height=2cm]{4}{root} 
	\depedge{4}{5}{mod} 
	\depedge{4}{6}{comp:aux} 
	\depedge{8}{7}{det} 
	\depedge{6}{8}{comp:obj} 
	\depedge[show label]{10}{9}{cc} 
	\depedge[show label, edge height=1.5cm]{4}{10}{conj} 
	\depedge[show label]{10}{11}{udep} 
	\depedge{11}{12}{comp:obj} 
	\end{dependency}
	
	\input{inputs/conllutable}
\end{exe}
\end{adjustwidth}
\vspace{.5cm}

Corpora in the CoNLL-U format were downloaded from the SUD website. If a corpus is large enough, it is split into three parts: training, development and testing. The training set is used to expose the parser to the correct dependency trees and based on that a prediction model is created. The model is tuned using the development set -- the model tries to predict what is the correct dependency tree for a sentence and the prediction is then confronted with the data in the set. Adjustments are made until there is no gain in the scores acheived by the parser for a certain number of learning steps (in the scripts used here this is 3000 steps). 

Training a model requires word vector data (which is provided with the default model for English) and a prepared treebank -- this means that all of the possible annotations from a treebank have to be listed for the prediction model to choose from. After all the needed files were provided, the training script was run. The batch size was set to 5000 and the dropout rate was 0.33. This means that the whole training dataset was split into batches, each with 5000 elements -- the bigger the batch the more precise the gradient function and thus the predictions. Dropout rate is the proportion of nodes in the neural network that are dropped during training. Without any dropout, a model might become overfitted for the training data. This means that it will be very good at predicting annotations for the sentences it has already seen, but not so much with any other data. The dropout rate chosen for training here was recommended in the training documentation.

All of the corpora used in this experiment were ready for training, besides one -- the GUMReddit corpus, as it is not fully available online. All of the dependencies and morphological features and so on are included in the CoNLL-U files, but the words themselves and the lemmas are missing. As the textual data is required for training, the appropriate script from the GUM corpus repository was utilised.\footnote{\url{https://github.com/amir-zeldes/gum/blob/master/get_text.py}} 

The following subsections describe the models that were trained as a part of this study.

\subsection{Combined model}
%ive got the links to sources in my obsidian notes (dobor procesorow i korpusow)

The most crucial of the models was the combined one. It was used to annotate most of the data analysed in this study. The corpora used for training were SUD\_English-EWT, SUD\_English-GUM, SUD\_English-ParTUT. 

% https://wstyler.ucsd.edu/enronsent/
The first of these is based on the English Web Treebank. It has 5 primary sources of data: weblogs, newsgroups, emails, reviews and question-answers. The first two -- weblogs and newsgroups -- were collected in the years 2003--2006. Email messages are from the Enronsent Corpus, which contains emails sent by the employees of Enron Corporation in the years 1999--2002, that were made public domain during the investigation of Enron. The emails in the corpus were processed to have as little non-human input as possible. The reviews of businesses and services included in the corpus were taken from various Google websites in 2011, but no specific dates were provided. The case was similar with question-answer data, which was collected in 2011 from the Yahoo! Answers website. In total, the corpus contains 254,820 words. The original constituency annotation was automatically converted into Stanford Dependencies and then manually corrected to UD. Using the code mentioned earlier in this chapter it was then converted to SUD. The corpus was chosen for training this model, as it is the biggest available annotated corpus for English. 

The second corpus used for this model was SUD\_English-GUM, which is a SUD-converted version of the Georgetown University Multilayer corpus. While the early versions of GUM were annotated in Stanford Dependencies scheme, they were later manually converted into UD and the later versions have been created natively in UD. The corpus comprises a variety of styles, for instance academic, interviews, travel guides, letters, how-to guides and forum discussions. The last of these requires special attention, since it is sometimes considered seperately from the rest of the GUM corpus. The source for this data are Reddit forum discussions, which means that the text within this part of the corpus is protected by the Reddit terms and conditions and thus not freely available. In this data, words are substituted by underscores in the form and lemma fields in CoNLL-U files. Using it for training requires first running a script that completed the corpus, as was mentioned eariler. Including the Reddit data, there are 228,399 tokens in the corpus, which makes it the second largest corpus among the ones available for English and an obvious choice for training a parser. 

%(about ParTUT -- how big it is, what data is it built on, how was is annotated natively, why did i choose it)
The third corpus used for training the combined model was one based on the English part of ParTUT, the multilingual parallel treebank from the University of Turin. It consists of legal texts, Wikipedia articles and transcriptions of TED Talks. It was originally annotated in a style specific to the treebanks developed at the University of Turin, then converted to UD and from that to SUD. The corpus has 49,602 tokens, which is a lot less compared to the corpora described earlier, but is still a significant contribution to the model, especially considering the style difference between this corpus and the other ones. 

One additional reason for choosing the corpora listed above is consistency of annotation. Some corpora, despite the style diversity they could provide for the parsing model, had to be excluded from training, because some information was missing or inconsistent with the other corpora used here. 

For English, the Stanza parser by default also uses a model trained on multiple corpora: EWT, GUM, PUD and Pronouns. The two last ones were not used to train the model here, as they were too small to have been split into the training, development and testing sets. 

\subsection{Spoken model}
Spoken and written language differ significantly, which can lead to poor automatic parsing performance. To avoid that, this experiment involves training a model specialising in spoken data. The corpora used to this end were SUD\_English-Atis and parts of the SUD\_English-GUM corpus. 

Parts of the GUM corpus utilised here were those with interviews, conversations and vlogs. The relevant sentences were extracted from the original CoNLL-U files by looking for documents labelled \texttt{id = GUM\_interview}, \texttt{id = GUM\_vlog} or \texttt{id = GUM\_conversation}. 

%(about Atis -- how big it is, what data is it built on, how was is annotated natively, why did i choose it)
As for the other corpus, Atis comprises sentences from the Airline Travel Informations dataset. Those are transcriptions of people asking automated inquiry systems for flight information. The corpus has 61,879 tokens and was natively annotated in UD. 

\subsection{Comparison models}
The main purpose of the combined model is to create annotation for analysis, though one of the intentions of this study was also to compare the learnability of the two dependency annotation schemes: the "semantic" UD and the "syntactic" SUD. However a direct comparison between the two combined models is not be possible for a lack of information about the UD combined model. The datasets included in the training are described in the Stanza documentation,\footnote{\url{https://stanfordnlp.github.io/stanza/combined_models.html}} however there is no information on how the datasets were split into training and testing sets. The splits are known for big corpora, such as GUM and EWT, but the PUD and Pronouns are too small to be split into smaller sets. Another issue is that the evaluation for the UD combined model was not made available and running an evaluation script on this model is not possible without the training, development and testing splits. 

Considering this, the learnability of the annotation schemes described here is compared based on the models that are trained on one corpus each. All of those models have been trained on corpora big enough to have official splits into training, development and testing sets. Evaluation of those models trained on UD is reported in the Stanza documentation and was conducted on the testing sets. The comparison SUD models were trained using the official splits for every corpus. Results of the evaluation of this training along with the evaluation results for the corresponding UD models are shown in Section (there will be a ref here).

\section{Data extraction}
%OMÓW TO NA PRZYKŁADZIE
%MOŻE DAJ WIĘCEJ CODE SNIPPETS?

The input data from the COCA corpus was in the form of text files. Those were split into sentences using another parser, Trankit \citep{nguyen2021trankit}, as the sentence segmentation outputs from Trankit and Stanza suggested that this would provide better quality data for further analysis. Split texts were then put into .tsv files, so that along with the sentence text, also the information about text identifier and the index of the sentence in a document could be included. Those sentences were then put into batches, but separated with \texttt{\textbackslash n\textbackslash n}. This way the parsing process was faster than it would be when parsing sentence by sentence, but the sentences were clearly already split. The next step was parsing the texts, which was done using a parsing pipeline. Creating a parsing pipeline requires specifying the input language and the processors that make up the pipeline -- in this case those were the tokeniser, part-of-speech-tagger, lemmatiser and dependency parser. Here the tokeniser and lemmatiser were those of the default English model, as those two processes are the same in UD and SUD. The other two processors, POS-tagger and dependency parser were trained on SUD and used in the pipeline. Additionally, it was specified that the text was already split into sentences by setting the \texttt{tokenize\_no\_split} parameter to \texttt{True}. The pipeline configuration is shown in (\ref{ex:pipeline}).

\begin{exe}
\ex\label{ex:pipeline}
\ttfamily\raggedright\small
config = \{\\\hspace{.5cm}
        'processors': 'tokenize,pos,lemma,depparse',\\\hspace{.5cm}
        'lang': 'en',\\\hspace{.5cm}
        'use\_gpu': True,\\\hspace{.5cm}
        'pos\_model\_path': './saved\_models/en\_combined-sud\_charlm\_tagger.pt',\\\hspace{.5cm}
        'depparse\_model\_path': './saved\_models/en\_combined-sud\_charlm\_parser.pt',\\\hspace{.5cm}
        'tokenize\_pretokenized': False,\\\hspace{.5cm}
        'tokenize\_no\_ssplit': True,\\\hspace{.5cm}
        'download\_method': stanza.DownloadMethod.REUSE\_RESOURCES\\\hspace{.5cm}
\}\\
nlp = stanza.Pipeline(**config)
\end{exe}

The process of extracting coordinations will be illustrated by the sentence shown in (\ref{ex:extract-sent1}) with a SUD tree.\footnote{Sentence \texttt{reviews-357217-0002} from the EWT corpus \citep{silveira14gold}.}
% not sure about this citation, should it be sth else?

\begin{exe}
\ex
\label{ex:extract-sent1}
\begin{dependency}
	\begin{deptext}
		 Incompetent\& servers\& ,\& kitchen\& and\& management\& .\\
	 \end{deptext} 
	\depedge{2}{1}{mod} 
	\deproot{2}{root} 
	\depedge{4}{3}{punct} 
	\depedge{2}{4}{conj} 
	\depedge{6}{5}{cc} 
	\depedge{4}{6}{conj} 
	\depedge[edge height=2cm]{2}{7}{punct} 
\end{dependency}
\end{exe}

The text is then given to the \texttt{nlp} pipeline and parsed. In the output there is a list of Sentence objects, which have a list of dependencies as one of their attributes. 

Coordinations are found by looking for \texttt{conj} dependencies within a sentence. The SUD approach assumes the chain representation of a coordination, therefore once there a \texttt{conj} dependency is found, the script looks for more links in the chain. In the sentence (\ref{ex:extract-sent1}) first the \texttt{conj} dependency will be found between the words \textsl{servers} and \textsl{kitchen}. Indices of those two words are saved in a list and the script looks through the dependencies attached to the word \textsl{kitchen}: there is \texttt{punct} and \texttt{conj}. The \texttt{conj} one is attached to the word \textsl{management}, therefore the same is then done for that word: its index is saved to the list and the script checks whether it has a \texttt{conj} dependency. Since it does not have such a dependency, the whole coordination has been found -- it has three conjuncts: \textsl{servers}, \textsl{kitchen} and \textsl{management}. 

Once all lists of indices representing coordinations in a given sentence are ready, for each one of them a dictionary is created. An example of such a dictionary will be presented for the coordination in the sentence in (\ref{ex:extract-sent2})\footnote{Sentence \texttt{answers-20111024111513AAAQhAO\_ans-0006} from the EWT corpus \citep{silveira14gold}.}.

\begin{exe}
\ex\label{ex:extract-sent2}
\begin{dependency}
	\begin{deptext}
		 Do\& yourself\& a\& favor\& and\& give\& them\& a\& call\& .\\
	 \end{deptext} 
	\deproot{1}{root} 
	\depedge{1}{2}{comp:obl} 
	\depedge{4}{3}{det} 
	\depedge{1}{4}{comp:obj} 
	\depedge{6}{5}{cc} 
	\depedge[edge height=2cm]{1}{6}{conj} 
	\depedge{6}{7}{comp:obl} 
	\depedge{9}{8}{det} 
	\depedge{6}{9}{comp:obj} 
	\depedge[edge height=3cm]{1}{10}{punct} 
\end{dependency}
\end{exe}

The first and last indices are popped from the list and added to the dictionary as the left and right conjunct heads respectively with the keys \texttt{'L'} and \texttt{'R'}. The rest of the conjuncts are added with the key \texttt{'other\_conjuncts'} -- for the coordination in sentence (\ref{ex:extract-sent2}) that list is empty, as there are only two conjuncts. This coordination does not have a governor, therefore the key \texttt{'gov'} is not added to the dictionary. The next step is looking for the conjunction of the coordination. If there is one, it is always attached to the right conjunct and it is labelled \texttt{cc}. Once it is found, the whole word is added to the dictionary with the \texttt{'conj'} key. 

Finally, the information about the conjunct lengths has to be extracted. All of the dependencies appearing between the left and right conjunct heads are considered private to the conjuncts they are attached to. This means that in sentence (\ref{ex:extract-sent2}) the case of the left conjunct is simple -- both of the dependencies of the word \textsl{Do} (\texttt{comp:obj} and \texttt{comp:obl}) are directed to the right and are therefore part of the left conjunct. As for the head of the right conjunct, the word \textsl{give}, its dependencies could be shared by the whole coordination. A heuristic applied here is that if any of the other conjuncts have a dependency with the same label as some external dependency of the leftmost or rightmost conjunct, that external dependency is private to this conjunct. Here, the word \textsl{give} has a dependency \texttt{comp:obj} directed to the right. The head of the other conjunct, \textsl{Do}, also has a dependency with this label, therefore this dependency is private to and part of the right conjunct. If the word \textsl{Do} did not have such a dependency, the word \textsl{call} would have to be excluded from the right conjunct and shared by the whole coordination. 

With conjunct texts found, they can be measured. This is done in characters by taking the length of the conjunct text, in words and tokens by counting how many of those parser found in the conjunct and in syllables using the \texttt{cmudict} from the \texttt{nltk} package. In (\ref{ex:coord-dict}) there are contents of the dictionary corresponding to the coordination found in sentence (\ref{ex:extract-sent2}).

\begin{exe}
\ex\label{ex:coord-dict}
\ttfamily\raggedright\scriptsize
\begin{multicols}{2}
'L': \{"id": 1,\\\hspace{.5cm}
  "text": "Do",\\\hspace{.5cm}
  "lemma": "do",\\\hspace{.5cm}
  "upos": "VERB",\\\hspace{.5cm}
  "xpos": "VB",\\\hspace{.5cm}
  "feats": "Mood=Imp|VerbForm=Fin",\\\hspace{.5cm}
  "head": 0,\\\hspace{.5cm}
  "deprel": "root"\}

'Lconj': 'Do yourself a favor'

'Lwords': 4

'Ltokens': 4

'Lsyl': 6

'R': \{"id": 6,\\\hspace{.5cm}
  "text": "give",\\\hspace{.5cm}
  "lemma": "give",\\\hspace{.5cm}
  "upos": "VERB",\\\hspace{.5cm}
  "xpos": "VB",\\\hspace{.5cm}
  "feats": "Mood=Imp|VerbForm=Fin",\\\hspace{.5cm}
  "head": 1,\\\hspace{.5cm}
  "deprel": "conj"\}

'Rconj': 'give them a call'

'Rwords': 4

'Rtokens': 4

'Rsyl': 4
\end{multicols}
\begin{multicols}{2}
'conj': \{"id": 5,\\\hspace{.5cm}
  "text": "and",\\\hspace{.5cm}
  "lemma": "and",\\\hspace{.5cm}
  "upos": "CCONJ",\\\hspace{.5cm}
  "xpos": "CC",\\\hspace{.5cm}
  "head": 6,\\\hspace{.5cm}
  "deprel": "cc"\}

'other\_conjuncts': []

'conj\_lengths': [(4, 19), (4, 16)]

'sentence': 'Do yourself a favor and give them a call.'

'sent\_id': 'answers-20111024111513AAAQhAO\_ans-0006'
\end{multicols}
\end{exe}

A list of dictionaries like the one in (\ref{ex:coord-dict}) is created for every processed corpus file. Then every dictionary becomes an observation in an output .csv table. 

